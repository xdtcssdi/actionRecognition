{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 原始模型\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import datasets, transforms\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader, Dataset, random_split, Subset\n",
    "from tqdm import tqdm,trange\n",
    "import torch.nn.functional as F\n",
    "import numpy as np \n",
    "from Datasets import AngleActionDatasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "classes = 5  #分类\n",
    "hidden_dim = 64 # rnn隐藏单元数\n",
    "lr = 0.001 # 学习率\n",
    "epoches = 20 #训练次数\n",
    "batch_size = 1024 # 每一个训练批次数量\n",
    "input_dim= 7 * 3 \n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "time_step = 40\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "训练集大小63980， 验证集大小15996\n"
     ]
    }
   ],
   "source": [
    "def create_data_loader():\n",
    "    train_data_path = \"D:\\\\temp\\\\attitudeAngle_windows\"\n",
    "    # train_data_path = \"D:\\\\temp\\\\augment_action_windows\"\n",
    "    datasets = AngleActionDatasets(train_data_path, transform=torch.tensor, target_transform=torch.tensor, pick_path='train.pkl')\n",
    "    test_datasets = AngleActionDatasets(\"D:\\\\temp\\\\yan1_action_windows\\\\attitudeAngle\", transform=torch.tensor, target_transform=torch.tensor, pick_path='test.pkl')\n",
    "    split_rate = 0.8  # 训练集占整个数据集的比例\n",
    "    train_len = int(split_rate * len(datasets))\n",
    "    valid_len = len(datasets) - train_len\n",
    "\n",
    "    train_sets, valid_sets = random_split(datasets, [train_len, valid_len], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "    train_loader = DataLoader(train_sets, batch_size=batch_size, shuffle=True,drop_last=True,pin_memory=True)\n",
    "    test_loader = DataLoader(test_datasets, shuffle=True,pin_memory=True)\n",
    "    valid_loader = DataLoader(valid_sets, batch_size=batch_size, shuffle=True,drop_last=True,pin_memory=True)\n",
    "\n",
    "    print(f\"训练集大小{len(train_sets)}， 验证集大小{len(valid_sets)}\")\n",
    "    return train_loader, test_loader, valid_loader\n",
    "train_loader, test_loader, valid_loader = create_data_loader()"
   ]
  },
  {
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim1, hidden_dim2, out_dim, time_step):\n",
    "        super(RNN, self).__init__()\n",
    "        self.time_step = time_step\n",
    "        self.hidden_dim2 = hidden_dim2\n",
    "        self.hidden_dim1 = hidden_dim1\n",
    "        self.linear1 = nn.Linear(input_dim, hidden_dim1)\n",
    "        self.linear2 = nn.Linear(input_dim, hidden_dim1)\n",
    "        self.linear3 = nn.Linear(hidden_dim2, time_step)\n",
    "        self.linear4 = nn.Linear(hidden_dim2, out_dim)\n",
    "\n",
    "        self.rnn1 = nn.LSTM(hidden_dim1, hidden_dim2, batch_first=True)\n",
    "        self.rnn2 = nn.LSTM(hidden_dim2, hidden_dim2, batch_first=True)\n",
    "        self.dp1 = nn.Dropout(p=0.3)\n",
    "        self.dp2 = nn.Dropout(p=0.3)\n",
    "        self.dp3 = nn.Dropout(p=0.6)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        # X.shape = batch_size, time_step, feature_num\n",
    "        A, B = X\n",
    "        batch_size = A.shape[0]\n",
    "        # out.shape = (batch_size, time_step, hidden_dim1)\n",
    "        out1 = self.dp1(F.relu(self.linear1(B)))\n",
    "        out2 = self.dp2(F.relu(self.linear2(A)))\n",
    "\n",
    "        # out1.shape = out.shape,  status1 = (h, c)\n",
    "        # h.shape = c.shape = (方向* 层数, batch_size, hidden_dim2)\n",
    "        out1, (h1, c1) = self.rnn1(out1)\n",
    "\n",
    "        h1 = h1.view(batch_size, self.hidden_dim2)\n",
    "        out11 = self.dp3(F.relu(self.linear3(h1)))\n",
    "\n",
    "        out11 = out11.unsqueeze(2).expand(batch_size, self.time_step, self.hidden_dim1)\n",
    "\n",
    "        out3 = out2 + out11\n",
    "        h2 = out3.mean(dim=1).unsqueeze(0)\n",
    "        out, _ = self.rnn2(out1, (h2, torch.zeros_like(h2)))\n",
    "\n",
    "        out = self.linear4(out[:,-1,:])\n",
    "\n",
    "        return out"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = RNN(input_dim, hidden_dim, hidden_dim, classes, time_step).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(rnn.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GETACC(loader=valid_loader):\n",
    "    rnn.eval()\n",
    "    cnt = 0\n",
    "    sum_valid_acc = 0\n",
    "    sum_valid_loss = 0\n",
    "    for data, label in loader:\n",
    "        data = [item.to(device) for item in data]\n",
    "        label = label.to(device)\n",
    "        out = rnn(data)\n",
    "        \n",
    "        _, predict = torch.max(out, 1)\n",
    "        \n",
    "        loss = criterion(out, label)\n",
    "        sum_valid_loss += loss.item()\n",
    "        acc = torch.mean((predict == label).float())\n",
    "        sum_valid_acc += acc\n",
    "        cnt+=1\n",
    "        \n",
    "    return sum_valid_loss/cnt, sum_valid_acc/cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "epoch = 0 train_loss = 1.6126844882965088 valid_loss = 1.607137934366862 valid_acc= 0.19993489980697632: 100%|██████████| 62/62 [00:07<00:00,  8.53it/s]\n",
      "epoch = 1 train_loss = 0.5170836448669434 valid_loss = 0.551896337668101 valid_acc= 0.8130208849906921: 100%|██████████| 62/62 [00:06<00:00,  8.93it/s]\n",
      "epoch = 2 train_loss = 0.3488447368144989 valid_loss = 0.3491211990515391 valid_acc= 0.8777344226837158: 100%|██████████| 62/62 [00:06<00:00,  9.66it/s]\n",
      "epoch = 3 train_loss = 0.2600637674331665 valid_loss = 0.27132836282253264 valid_acc= 0.90130215883255: 100%|██████████| 62/62 [00:06<00:00,  9.74it/s]\n",
      "epoch = 4 train_loss = 0.2478613257408142 valid_loss = 0.22089275519053142 valid_acc= 0.9191406965255737: 100%|██████████| 62/62 [00:06<00:00,  9.97it/s]\n",
      "epoch = 5 train_loss = 0.17582334578037262 valid_loss = 0.19349854389826457 valid_acc= 0.9263021349906921: 100%|██████████| 62/62 [00:06<00:00, 10.26it/s]\n",
      "epoch = 6 train_loss = 0.16198143362998962 valid_loss = 0.17846474051475525 valid_acc= 0.9320312738418579: 100%|██████████| 62/62 [00:06<00:00, 10.13it/s]\n",
      "epoch = 7 train_loss = 0.17385733127593994 valid_loss = 0.16655778686205547 valid_acc= 0.9361979365348816: 100%|██████████| 62/62 [00:06<00:00,  9.96it/s]\n",
      "epoch = 8 train_loss = 0.1574317365884781 valid_loss = 0.15601744453112285 valid_acc= 0.9378255605697632: 100%|██████████| 62/62 [00:06<00:00,  9.99it/s]\n",
      "epoch = 9 train_loss = 0.11964403092861176 valid_loss = 0.14625982145468394 valid_acc= 0.9401041865348816: 100%|██████████| 62/62 [00:06<00:00,  9.93it/s]\n",
      "epoch = 10 train_loss = 0.11748775839805603 valid_loss = 0.13828375041484833 valid_acc= 0.9444010853767395: 100%|██████████| 62/62 [00:06<00:00,  9.95it/s]\n",
      "epoch = 11 train_loss = 0.1296575516462326 valid_loss = 0.13307648052771887 valid_acc= 0.9457031488418579: 100%|██████████| 62/62 [00:06<00:00, 10.03it/s]\n",
      "epoch = 12 train_loss = 0.12199118733406067 valid_loss = 0.12791881014903386 valid_acc= 0.9482422471046448: 100%|██████████| 62/62 [00:06<00:00,  9.84it/s]\n",
      "epoch = 13 train_loss = 0.10032285749912262 valid_loss = 0.12137050131956736 valid_acc= 0.9498698115348816: 100%|██████████| 62/62 [00:06<00:00,  9.68it/s]\n",
      "epoch = 14 train_loss = 0.09289729595184326 valid_loss = 0.12399373402198156 valid_acc= 0.94720059633255: 100%|██████████| 62/62 [00:06<00:00,  9.88it/s]\n",
      "epoch = 15 train_loss = 0.0962454080581665 valid_loss = 0.12702421794335048 valid_acc= 0.94817715883255: 100%|██████████| 62/62 [00:06<00:00,  9.76it/s]\n",
      "epoch = 16 train_loss = 0.09740957617759705 valid_loss = 0.12274025877316792 valid_acc= 0.94915372133255: 100%|██████████| 62/62 [00:06<00:00,  9.77it/s]\n",
      "epoch = 17 train_loss = 0.08836961537599564 valid_loss = 0.11274913251399994 valid_acc= 0.9526042342185974: 100%|██████████| 62/62 [00:06<00:00,  9.84it/s]\n",
      "epoch = 18 train_loss = 0.07434015721082687 valid_loss = 0.11207079490025838 valid_acc= 0.9552735090255737: 100%|██████████| 62/62 [00:06<00:00,  9.83it/s]\n",
      "epoch = 19 train_loss = 0.07545123249292374 valid_loss = 0.1108173926671346 valid_acc= 0.9558594226837158: 100%|██████████| 62/62 [00:06<00:00,  9.72it/s]\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epoches):\n",
    "    i = 0\n",
    "    loss_sum = 0\n",
    "    bar = tqdm(train_loader)\n",
    "    for ii, (data , label) in enumerate(bar):\n",
    "        rnn.train()\n",
    "    \n",
    "        data = [item.to(device) for item in data]\n",
    "        label = label.to(device)\n",
    "        out = rnn(data)\n",
    "        loss = criterion(out, label)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        i+=1\n",
    "        loss_sum += loss.item()\n",
    "\n",
    "        if ii == 0:\n",
    "            valid_loss,valid_acc = GETACC(valid_loader)\n",
    "            \n",
    "            bar.set_description(f\"epoch = {epoch} train_loss = {loss_sum/i} valid_loss = {valid_loss} valid_acc= {valid_acc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "test_loss = 1.0351570993453665, test_acc = 0.7542288899421692\n"
     ]
    }
   ],
   "source": [
    "test_loss,test_acc = GETACC(test_loader)\n",
    "print(f\"test_loss = {test_loss}, test_acc = {test_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}