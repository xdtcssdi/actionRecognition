{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 原始模型\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import datasets, transforms\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader, Dataset, random_split, Subset\n",
    "from tqdm import tqdm,trange\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActionDatasets(Dataset):\n",
    "    def __init__(self, csv_path, transform=None, target_transform=None, pick_path = \"data.pkl\"):\n",
    "        super(ActionDatasets, self).__init__()\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        import pandas as pd\n",
    "        from glob import glob\n",
    "        import os\n",
    "        csvs = glob(os.path.join(csv_path, \"*.csv\"))\n",
    "\n",
    "        if len(csvs) == 0:\n",
    "            raise ValueError(\"路径下不存在csv文件\")\n",
    "            return\n",
    "        df = []\n",
    "\n",
    "        if os.path.exists(pick_path):\n",
    "            df = pd.read_pickle(pick_path)\n",
    "        else:\n",
    "            for label, csv in enumerate(csvs):\n",
    "                if type(df) == list:\n",
    "                    df = pd.read_csv(csv)\n",
    "                    df['label'] = label\n",
    "                else:\n",
    "                    df_tmp = pd.read_csv(csv)\n",
    "                    df_tmp['label'] = label\n",
    "                    df = pd.concat([df, df_tmp])\n",
    "            df.to_pickle(pick_path)\n",
    "            \n",
    "        self.A_values = df[['aAX', 'aAY', 'aAZ','bAX', 'bAY', 'bAZ', 'cAX', 'cAY', 'cAZ', 'dAX', 'dAY', 'dAZ', 'eAX', 'eAY', 'eAZ', 'fAX', 'fAY', 'fAZ', 'gAX', 'gAY', 'gAZ']].values\n",
    "\n",
    "        self.B_values = df[['aWX', 'aWY', 'aWZ','bWX', 'bWY', 'bWZ', 'cWX', 'cWY', 'cWZ', 'dWX', 'dWY', 'dWZ', 'eWX', 'eWY', 'eWZ', 'fWX', 'fWY', 'fWZ', 'gWX', 'gWY', 'gWZ']].values\n",
    "        self.labels_values = df[['label']].values\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "            \n",
    "        A_data = torch.tensor(self.A_values[idx*40:(idx+1)*40, :],dtype=torch.float32)\n",
    "        B_data = torch.tensor(self.B_values[idx*40:(idx+1)*40, :],dtype=torch.float32)\n",
    "        label_data = torch.tensor(self.labels_values[idx*40:(idx+1)*40, -1][0], dtype=torch.long)\n",
    "\n",
    "        if self.transform:\n",
    "            A_data = self.transform(A_data)\n",
    "            B_data = self.transform(B_data)\n",
    "        if self.target_transform:\n",
    "            label_data = self.target_transform(label_data)\n",
    "\n",
    "        return (A_data, B_data), label_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.labels_values.shape[0]//40\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "classes = 5  #分类\n",
    "hidden_dim = 64 # rnn隐藏单元数\n",
    "lr = 0.001 # 学习率\n",
    "epoches = 20 #训练次数\n",
    "batch_size = 128 # 每一个训练批次数量\n",
    "input_dim= 7 * 3 \n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "time_step = 40\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "训练集大小63980， 验证集大小15996\n"
     ]
    }
   ],
   "source": [
    "def create_data_loader():\n",
    "    train_data_path = \"D:\\\\temp\\\\augment_action_windows\"\n",
    "    # train_data_path = \"D:\\\\temp\\\\augment_action_windows\"\n",
    "    datasets = ActionDatasets(train_data_path, transform=torch.tensor, target_transform=torch.tensor, pick_path='train.pkl')\n",
    "    test_datasets = ActionDatasets(\"D:\\\\temp\\\\yan1_action_windows\\\\action_windows\", transform=torch.tensor, target_transform=torch.tensor, pick_path='test.pkl')\n",
    "    split_rate = 0.8  # 训练集占整个数据集的比例\n",
    "    train_len = int(split_rate * len(datasets))\n",
    "    valid_len = len(datasets) - train_len\n",
    "\n",
    "    train_sets, valid_sets = random_split(datasets, [train_len, valid_len], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "    train_loader = DataLoader(train_sets, batch_size=batch_size, shuffle=True,drop_last=True,pin_memory=True)\n",
    "    test_loader = DataLoader(test_datasets, batch_size=batch_size, shuffle=True,drop_last=True,pin_memory=True)\n",
    "    valid_loader = DataLoader(valid_sets, batch_size=batch_size, shuffle=True,drop_last=True,pin_memory=True)\n",
    "\n",
    "    print(f\"训练集大小{len(train_sets)}， 验证集大小{len(valid_sets)}\")\n",
    "    return train_loader, test_loader, valid_loader\n",
    "train_loader, test_loader, valid_loader = create_data_loader()"
   ]
  },
  {
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim1, hidden_dim2, out_dim, time_step):\n",
    "        super(RNN, self).__init__()\n",
    "        self.time_step = time_step\n",
    "        self.hidden_dim2 = hidden_dim2\n",
    "        self.hidden_dim1 = hidden_dim1\n",
    "        self.linear1 = nn.Linear(input_dim, hidden_dim1)\n",
    "        self.linear2 = nn.Linear(input_dim, hidden_dim1)\n",
    "        self.linear3 = nn.Linear(hidden_dim2, time_step)\n",
    "        self.linear4 = nn.Linear(hidden_dim2, out_dim)\n",
    "\n",
    "        self.rnn1 = nn.LSTM(hidden_dim1, hidden_dim2, batch_first=True)\n",
    "        self.rnn2 = nn.LSTM(hidden_dim2, hidden_dim2, batch_first=True)\n",
    "        self.dp1 = nn.Dropout(p=0.5)\n",
    "        self.dp2 = nn.Dropout(p=0.5)\n",
    "        self.dp3 = nn.Dropout(p=0.5)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        # X.shape = batch_size, time_step, feature_num\n",
    "        A, B = X\n",
    "        batch_size = A.shape[0]\n",
    "        # out.shape = (batch_size, time_step, hidden_dim1)\n",
    "        out1 = self.dp1(F.relu(self.linear1(B)))\n",
    "        out2 = self.dp2(F.relu(self.linear2(A)))\n",
    "\n",
    "        # out1.shape = out.shape,  status1 = (h, c)\n",
    "        # h.shape = c.shape = (方向* 层数, batch_size, hidden_dim2)\n",
    "        out1, (h1, c1) = self.rnn1(out1)\n",
    "\n",
    "        h1 = h1.view(batch_size, self.hidden_dim2)\n",
    "        out11 = self.dp3(F.relu(self.linear3(h1)))\n",
    "\n",
    "        out11 = out11.unsqueeze(2).expand(batch_size, self.time_step, self.hidden_dim1)\n",
    "\n",
    "        out3 = out2 + out11\n",
    "        h2 = out3.mean(dim=1).unsqueeze(0)\n",
    "        out, _ = self.rnn2(out1, (h2, torch.zeros_like(h2)))\n",
    "\n",
    "        out = self.linear4(out[:,-1,:])\n",
    "\n",
    "        return out"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = RNN(input_dim, hidden_dim, hidden_dim, classes, time_step).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(rnn.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GETACC(loader=valid_loader):\n",
    "    rnn.eval()\n",
    "    cnt = 0\n",
    "    sum_valid_acc = 0\n",
    "    sum_valid_loss = 0\n",
    "    for data, label in loader:\n",
    "        data = [item.to(device) for item in data]\n",
    "        label = label.to(device)\n",
    "        out = rnn(data)\n",
    "        \n",
    "        _, predict = torch.max(out, 1)\n",
    "        \n",
    "        loss = criterion(out, label)\n",
    "        sum_valid_loss += loss.item()\n",
    "        acc = torch.sum((predict == label).int()) / batch_size\n",
    "        sum_valid_acc += acc\n",
    "        cnt+=1\n",
    "    \n",
    "    return sum_valid_loss/cnt, sum_valid_acc/cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "epoch = 0 train_loss = 1.61309814453125 valid_loss = 1.6148927567466613 valid_acc= 0.1846018135547638: 100%|██████████| 499/499 [00:10<00:00, 46.15it/s]\n",
      "epoch = 1 train_loss = 0.43142569065093994 valid_loss = 0.43095781041249154 valid_acc= 0.8332282900810242: 100%|██████████| 499/499 [00:09<00:00, 50.03it/s]\n",
      "epoch = 2 train_loss = 0.19101852178573608 valid_loss = 0.2991094304428947 valid_acc= 0.8884198069572449: 100%|██████████| 499/499 [00:09<00:00, 50.22it/s]\n",
      "epoch = 3 train_loss = 0.168981671333313 valid_loss = 0.22897810573058744 valid_acc= 0.9103452563285828: 100%|██████████| 499/499 [00:09<00:00, 50.87it/s]\n",
      "epoch = 4 train_loss = 0.21631953120231628 valid_loss = 0.1919516451176136 valid_acc= 0.9179057478904724: 100%|██████████| 499/499 [00:09<00:00, 50.43it/s]\n",
      "epoch = 5 train_loss = 0.18143948912620544 valid_loss = 0.1723446006616277 valid_acc= 0.9287424087524414: 100%|██████████| 499/499 [00:09<00:00, 50.39it/s]\n",
      "epoch = 6 train_loss = 0.09578686952590942 valid_loss = 0.15600681449136428 valid_acc= 0.9358618855476379: 100%|██████████| 499/499 [00:10<00:00, 49.69it/s]\n",
      "epoch = 7 train_loss = 0.140871062874794 valid_loss = 0.14002590283991828 valid_acc= 0.9426032900810242: 100%|██████████| 499/499 [00:09<00:00, 49.91it/s]\n",
      "epoch = 8 train_loss = 0.1603027880191803 valid_loss = 0.13141036499291658 valid_acc= 0.9468245506286621: 100%|██████████| 499/499 [00:10<00:00, 49.21it/s]\n",
      "epoch = 9 train_loss = 0.07215286046266556 valid_loss = 0.12694393215520727 valid_acc= 0.9499117732048035: 100%|██████████| 499/499 [00:09<00:00, 50.90it/s]\n",
      "epoch = 10 train_loss = 0.08532044291496277 valid_loss = 0.12930173378798268 valid_acc= 0.9495967626571655: 100%|██████████| 499/499 [00:09<00:00, 50.58it/s]\n",
      "epoch = 11 train_loss = 0.0733397901058197 valid_loss = 0.11535241611061557 valid_acc= 0.9535660147666931: 100%|██████████| 499/499 [00:09<00:00, 50.87it/s]\n",
      "epoch = 12 train_loss = 0.06353574246168137 valid_loss = 0.11373605971194563 valid_acc= 0.9553301334381104: 100%|██████████| 499/499 [00:09<00:00, 50.31it/s]\n",
      "epoch = 13 train_loss = 0.14304785430431366 valid_loss = 0.10599123605436855 valid_acc= 0.958102285861969: 100%|██████████| 499/499 [00:09<00:00, 49.98it/s]\n",
      "epoch = 14 train_loss = 0.10739080607891083 valid_loss = 0.11152363865966758 valid_acc= 0.9563381671905518: 100%|██████████| 499/499 [00:09<00:00, 50.02it/s]\n",
      "epoch = 15 train_loss = 0.05480493605136871 valid_loss = 0.1058126832809179 valid_acc= 0.9598664045333862: 100%|██████████| 499/499 [00:09<00:00, 50.15it/s]\n",
      "epoch = 16 train_loss = 0.013401144184172153 valid_loss = 0.10223072253528141 valid_acc= 0.9582912921905518: 100%|██████████| 499/499 [00:09<00:00, 50.46it/s]\n",
      "epoch = 17 train_loss = 0.1104491725564003 valid_loss = 0.11189679383871055 valid_acc= 0.9567161798477173: 100%|██████████| 499/499 [00:09<00:00, 51.03it/s]\n",
      "epoch = 18 train_loss = 0.05656473711133003 valid_loss = 0.09727860662725664 valid_acc= 0.9610635042190552: 100%|██████████| 499/499 [00:09<00:00, 49.97it/s]\n",
      "epoch = 19 train_loss = 0.06523006409406662 valid_loss = 0.10579712027984281 valid_acc= 0.9582912921905518: 100%|██████████| 499/499 [00:09<00:00, 50.01it/s]\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epoches):\n",
    "    i = 0\n",
    "    loss_sum = 0\n",
    "    bar = tqdm(train_loader)\n",
    "    for ii, (data , label) in enumerate(bar):\n",
    "        rnn.train()\n",
    "    \n",
    "        data = [item.to(device) for item in data]\n",
    "        label = label.to(device)\n",
    "        out = rnn(data)\n",
    "        loss = criterion(out, label)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        i+=1\n",
    "        loss_sum += loss.item()\n",
    "\n",
    "        if ii == 0:\n",
    "            valid_loss,valid_acc = GETACC(valid_loader)\n",
    "            \n",
    "            bar.set_description(f\"epoch = {epoch} train_loss = {loss_sum/i} valid_loss = {valid_loss} valid_acc= {valid_acc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "test_loss = 0.8903724891798837, test_acc = 0.7511160969734192\n"
     ]
    }
   ],
   "source": [
    "test_loss,test_acc = GETACC(test_loader)\n",
    "print(f\"test_loss = {test_loss}, test_acc = {test_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}