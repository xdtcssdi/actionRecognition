{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 原始模型\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import datasets, transforms\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader, Dataset, random_split, Subset\n",
    "from tqdm import tqdm,trange\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActionDatasets(Dataset):\n",
    "    def __init__(self, csv_path, transform=None, target_transform=None, pick_path = \"data.pkl\"):\n",
    "        super(ActionDatasets, self).__init__()\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        import pandas as pd\n",
    "        from glob import glob\n",
    "        import os\n",
    "        csvs = glob(os.path.join(csv_path, \"*.csv\"))\n",
    "\n",
    "        if len(csvs) == 0:\n",
    "            raise ValueError(\"路径下不存在csv文件\")\n",
    "            return\n",
    "        df = []\n",
    "\n",
    "        if os.path.exists(pick_path):\n",
    "            df = pd.read_pickle(pick_path)\n",
    "        else:\n",
    "            for label, csv in enumerate(csvs):\n",
    "                if type(df) == list:\n",
    "                    df = pd.read_csv(csv)\n",
    "                    df['label'] = label\n",
    "                else:\n",
    "                    df_tmp = pd.read_csv(csv)\n",
    "                    df_tmp['label'] = label\n",
    "                    df = pd.concat([df, df_tmp])\n",
    "            df.to_pickle(pick_path)\n",
    "        self.data = df\n",
    "        self.values = self.data.values\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        train_data, label_data = torch.tensor(self.values[idx*40:(idx+1)*40, 1:-2],dtype=torch.float32) ,torch.tensor(self.values[idx*40:(idx+1)*40, -1][0], dtype=torch.long)\n",
    "\n",
    "        if self.transform:\n",
    "            train_data = self.transform(train_data)\n",
    "        if self.target_transform:\n",
    "            label_data = self.target_transform(label_data)\n",
    "\n",
    "        return train_data ,label_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)//40\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "classes = 5  #分类\n",
    "hidden_dim = 64 # rnn隐藏单元数\n",
    "lr = 0.001 # 学习率\n",
    "epoches = 20 #训练次数\n",
    "batch_size = 128 # 每一个训练批次数量\n",
    "input_dim= 42\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "训练集大小63980， 验证集大小15996\n"
     ]
    }
   ],
   "source": [
    "def create_data_loader():\n",
    "    train_data_path = \"D:\\\\temp\\\\augment_action_windows\"\n",
    "    # train_data_path = \"D:\\\\temp\\\\augment_action_windows\"\n",
    "    datasets = ActionDatasets(train_data_path, transform=torch.tensor, target_transform=torch.tensor, pick_path='train.pkl')\n",
    "    test_datasets = ActionDatasets(\"D:\\\\temp\\\\yan1_action_windows\\\\action_windows\", transform=torch.tensor, target_transform=torch.tensor, pick_path='test.pkl')\n",
    "    split_rate = 0.8  # 训练集占整个数据集的比例\n",
    "    train_len = int(split_rate * len(datasets))\n",
    "    valid_len = len(datasets) - train_len\n",
    "\n",
    "    train_sets, valid_sets = random_split(datasets, [train_len, valid_len], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "    train_loader = DataLoader(train_sets, batch_size=batch_size, shuffle=True,drop_last=True,pin_memory=True)\n",
    "    test_loader = DataLoader(test_datasets, batch_size=batch_size, shuffle=True,drop_last=True,pin_memory=True)\n",
    "    valid_loader = DataLoader(valid_sets, batch_size=batch_size, shuffle=True,drop_last=True,pin_memory=True)\n",
    "\n",
    "    print(f\"训练集大小{len(train_sets)}， 验证集大小{len(valid_sets)}\")\n",
    "    return train_loader, test_loader, valid_loader\n",
    "train_loader, test_loader, valid_loader = create_data_loader()"
   ]
  },
  {
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, out_dim):\n",
    "        super(RNN, self).__init__()\n",
    "        self.rnn = nn.LSTM(input_dim, hidden_dim, 3, batch_first=True)\n",
    "        self.dropout1 = nn.Dropout(p=0.3)\n",
    "        \n",
    "        self.linear1 = nn.Linear(hidden_dim, 64)\n",
    "        self.linear2 = nn.Linear(64, out_dim)\n",
    "    def forward(self, X):\n",
    "        out, status = self.rnn(X)\n",
    "        out = F.relu(out[:,-1,:])\n",
    "        out = F.relu(self.linear1(out))\n",
    "        out = self.dropout1(out)\n",
    "        out = self.linear2(out)\n",
    "\n",
    "        return out"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = RNN(input_dim, hidden_dim, classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(rnn.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GETACC(loader=valid_loader):\n",
    "    rnn.eval()\n",
    "    cnt = 0\n",
    "    sum_valid_acc = 0\n",
    "    sum_valid_loss = 0\n",
    "    for data, label in loader:\n",
    "        data = data.to(device)\n",
    "        label = label.to(device)\n",
    "        out = rnn(data)\n",
    "        \n",
    "        _, predict = torch.max(out, 1)\n",
    "        \n",
    "        loss = criterion(out, label)\n",
    "        sum_valid_loss += loss.item()\n",
    "        acc = torch.sum((predict == label).int()) / batch_size\n",
    "        sum_valid_acc += acc\n",
    "        cnt+=1\n",
    "    \n",
    "    return sum_valid_loss/cnt, sum_valid_acc/cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "epoch = 0 train_loss = 1.6120792627334595 valid_loss = 1.607880014565683 valid_acc= 0.19959676265716553: 100%|██████████| 499/499 [00:10<00:00, 47.94it/s]\n",
      "epoch = 1 train_loss = 0.3518986403942108 valid_loss = 0.3434682425952727 valid_acc= 0.8690776228904724: 100%|██████████| 499/499 [00:09<00:00, 52.60it/s]\n",
      "epoch = 2 train_loss = 0.2689221203327179 valid_loss = 0.25528301845394796 valid_acc= 0.8986895084381104: 100%|██████████| 499/499 [00:09<00:00, 52.62it/s]\n",
      "epoch = 3 train_loss = 0.25424203276634216 valid_loss = 0.2247106836688134 valid_acc= 0.9061239361763: 100%|██████████| 499/499 [00:09<00:00, 53.16it/s]\n",
      "epoch = 4 train_loss = 0.19590070843696594 valid_loss = 0.20892722862622432 valid_acc= 0.9157635569572449: 100%|██████████| 499/499 [00:09<00:00, 52.95it/s]\n",
      "epoch = 5 train_loss = 0.11097963154315948 valid_loss = 0.1984835509210825 valid_acc= 0.9165826439857483: 100%|██████████| 499/499 [00:09<00:00, 52.18it/s]\n",
      "epoch = 6 train_loss = 0.10062102228403091 valid_loss = 0.18756664159797853 valid_acc= 0.9213709235191345: 100%|██████████| 499/499 [00:09<00:00, 52.41it/s]\n",
      "epoch = 7 train_loss = 0.1469370275735855 valid_loss = 0.18774855455323572 valid_acc= 0.9236390590667725: 100%|██████████| 499/499 [00:09<00:00, 53.22it/s]\n",
      "epoch = 8 train_loss = 0.13821302354335785 valid_loss = 0.18713072843609319 valid_acc= 0.9224420189857483: 100%|██████████| 499/499 [00:09<00:00, 52.10it/s]\n",
      "epoch = 9 train_loss = 0.1921602487564087 valid_loss = 0.18041861604057974 valid_acc= 0.9215599298477173: 100%|██████████| 499/499 [00:09<00:00, 51.84it/s]\n",
      "epoch = 10 train_loss = 0.09924770891666412 valid_loss = 0.17436120190447377 valid_acc= 0.926852285861969: 100%|██████████| 499/499 [00:09<00:00, 52.21it/s]\n",
      "epoch = 11 train_loss = 0.13480031490325928 valid_loss = 0.16821794836751877 valid_acc= 0.9283643960952759: 100%|██████████| 499/499 [00:09<00:00, 51.77it/s]\n",
      "epoch = 12 train_loss = 0.16489258408546448 valid_loss = 0.1665770815264794 valid_acc= 0.9278603792190552: 100%|██████████| 499/499 [00:09<00:00, 51.71it/s]\n",
      "epoch = 13 train_loss = 0.12961894273757935 valid_loss = 0.16831660195584258 valid_acc= 0.9334676861763: 100%|██████████| 499/499 [00:09<00:00, 51.49it/s]\n",
      "epoch = 14 train_loss = 0.08095554262399673 valid_loss = 0.1563494795813195 valid_acc= 0.9337827563285828: 100%|██████████| 499/499 [00:09<00:00, 52.78it/s]\n",
      "epoch = 15 train_loss = 0.05715230107307434 valid_loss = 0.17276522415059228 valid_acc= 0.9315776228904724: 100%|██████████| 499/499 [00:09<00:00, 51.89it/s]\n",
      "epoch = 16 train_loss = 0.0854993611574173 valid_loss = 0.1661826815275896 valid_acc= 0.9350427985191345: 100%|██████████| 499/499 [00:09<00:00, 51.96it/s]\n",
      "epoch = 17 train_loss = 0.07231374830007553 valid_loss = 0.15629751451553836 valid_acc= 0.9347907900810242: 100%|██████████| 499/499 [00:09<00:00, 52.82it/s]\n",
      "epoch = 18 train_loss = 0.09691575169563293 valid_loss = 0.1707703888175949 valid_acc= 0.9320816397666931: 100%|██████████| 499/499 [00:09<00:00, 52.26it/s]\n",
      "epoch = 19 train_loss = 0.09326885640621185 valid_loss = 0.15648842503827426 valid_acc= 0.9372479319572449: 100%|██████████| 499/499 [00:09<00:00, 51.79it/s]\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epoches):\n",
    "    i = 0\n",
    "    loss_sum = 0\n",
    "    bar = tqdm(train_loader)\n",
    "    for ii, (data , label) in enumerate(bar):\n",
    "        rnn.train()\n",
    "        data = data.to(device)\n",
    "        label = label.to(device)\n",
    "        out = rnn(data)\n",
    "        loss = criterion(out, label)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        i+=1\n",
    "        loss_sum += loss.item()\n",
    "\n",
    "        if ii == 0:\n",
    "            valid_loss,valid_acc = GETACC(valid_loader)\n",
    "            \n",
    "            bar.set_description(f\"epoch = {epoch} train_loss = {loss_sum/i} valid_loss = {valid_loss} valid_acc= {valid_acc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "test_loss = 0.8597071766853333, test_acc = 0.7957589626312256\n"
     ]
    }
   ],
   "source": [
    "test_loss,test_acc = GETACC(test_loader)\n",
    "print(f\"test_loss = {test_loss}, test_acc = {test_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}