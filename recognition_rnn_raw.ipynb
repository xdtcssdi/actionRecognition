{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 原始模型\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import datasets, transforms\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader, Dataset, random_split, Subset\n",
    "from tqdm import tqdm,trange\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActionDatasets(Dataset):\n",
    "    def __init__(self, csv_path, transform=None, target_transform=None, pick_path = \"data.pkl\"):\n",
    "        super(ActionDatasets, self).__init__()\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        import pandas as pd\n",
    "        from glob import glob\n",
    "        import os\n",
    "        csvs = glob(os.path.join(csv_path, \"*.csv\"))\n",
    "\n",
    "        if len(csvs) == 0:\n",
    "            raise ValueError(\"路径下不存在csv文件\")\n",
    "            return\n",
    "        df = []\n",
    "\n",
    "        if os.path.exists(pick_path):\n",
    "            df = pd.read_pickle(pick_path)\n",
    "        else:\n",
    "            for label, csv in enumerate(csvs):\n",
    "                if type(df) == list:\n",
    "                    df = pd.read_csv(csv)\n",
    "                    df['label'] = label\n",
    "                else:\n",
    "                    df_tmp = pd.read_csv(csv)\n",
    "                    df_tmp['label'] = label\n",
    "                    df = pd.concat([df, df_tmp])\n",
    "            df.to_pickle(pick_path)\n",
    "        self.data = df\n",
    "        self.values = self.data.values\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        train_data, label_data = torch.tensor(self.values[idx*40:(idx+1)*40, 1:-2],dtype=torch.float32) ,torch.tensor(self.values[idx*40:(idx+1)*40, -1][0], dtype=torch.long)\n",
    "\n",
    "        if self.transform:\n",
    "            train_data = self.transform(train_data)\n",
    "        if self.target_transform:\n",
    "            label_data = self.target_transform(label_data)\n",
    "\n",
    "        return train_data ,label_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)//40\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "classes = 5  #分类\n",
    "hidden_dim = 64 # rnn隐藏单元数\n",
    "lr = 0.001 # 学习率\n",
    "epoches = 20 #训练次数\n",
    "batch_size = 128 # 每一个训练批次数量\n",
    "input_dim= 42\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "训练集大小7997， 验证集大小2000\n"
     ]
    }
   ],
   "source": [
    "def create_data_loader():\n",
    "    train_data_path = \"D:\\\\temp\\\\action_windows-test\"\n",
    "    # train_data_path = \"D:\\\\temp\\\\augment_action_windows\"\n",
    "    datasets = ActionDatasets(train_data_path, transform=torch.tensor, target_transform=torch.tensor, pick_path='train.pkl')\n",
    "    test_datasets = ActionDatasets(\"D:\\\\temp\\\\yan1_action_windows\\\\action_windows\", transform=torch.tensor, target_transform=torch.tensor, pick_path='test.pkl')\n",
    "    split_rate = 0.8  # 训练集占整个数据集的比例\n",
    "    train_len = int(split_rate * len(datasets))\n",
    "    valid_len = len(datasets) - train_len\n",
    "\n",
    "    train_sets, valid_sets = random_split(datasets, [train_len, valid_len], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "    train_loader = DataLoader(train_sets, batch_size=batch_size, shuffle=True,drop_last=True,pin_memory=True)\n",
    "    test_loader = DataLoader(test_datasets, batch_size=batch_size, shuffle=True,drop_last=True,pin_memory=True)\n",
    "    valid_loader = DataLoader(valid_sets, batch_size=batch_size, shuffle=True,drop_last=True,pin_memory=True)\n",
    "\n",
    "    print(f\"训练集大小{len(train_sets)}， 验证集大小{len(valid_sets)}\")\n",
    "    return train_loader, test_loader, valid_loader\n",
    "train_loader, test_loader, valid_loader = create_data_loader()"
   ]
  },
  {
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, out_dim):\n",
    "        super(RNN, self).__init__()\n",
    "        self.rnn = nn.LSTM(input_dim, hidden_dim, 3, batch_first=True)\n",
    "        self.dropout1 = nn.Dropout(p=0.3)\n",
    "        \n",
    "        self.linear1 = nn.Linear(hidden_dim, 64)\n",
    "        self.linear2 = nn.Linear(64, out_dim)\n",
    "    def forward(self, X):\n",
    "        out, status = self.rnn(X)\n",
    "        out = F.relu(out[:,-1,:])\n",
    "        out = F.relu(self.linear1(out))\n",
    "        out = self.dropout1(out)\n",
    "        out = self.linear2(out)\n",
    "\n",
    "        return out"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = RNN(input_dim, hidden_dim, classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(rnn.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GETACC(loader=valid_loader):\n",
    "    rnn.eval()\n",
    "    cnt = 0\n",
    "    sum_valid_acc = 0\n",
    "    sum_valid_loss = 0\n",
    "    for data, label in loader:\n",
    "        data = data.to(device)\n",
    "        label = label.to(device)\n",
    "        out = rnn(data)\n",
    "        \n",
    "        _, predict = torch.max(out, 1)\n",
    "        \n",
    "        loss = criterion(out, label)\n",
    "        sum_valid_loss += loss.item()\n",
    "        acc = torch.sum((predict == label).int()) / batch_size\n",
    "        sum_valid_acc += acc\n",
    "        cnt+=1\n",
    "    \n",
    "    return sum_valid_loss/cnt, sum_valid_acc/cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "epoch = 0 train_loss = 1.6094690561294556 valid_loss = 1.6099004030227662 valid_acc= 0.21406251192092896: 100%|██████████| 62/62 [00:02<00:00, 30.16it/s]\n",
      "epoch = 1 train_loss = 0.5264178514480591 valid_loss = 0.4968678553899129 valid_acc= 0.8359375596046448: 100%|██████████| 62/62 [00:01<00:00, 54.75it/s]\n",
      "epoch = 2 train_loss = 0.12979017198085785 valid_loss = 0.13718143552541734 valid_acc= 0.9645833969116211: 100%|██████████| 62/62 [00:01<00:00, 51.78it/s]\n",
      "epoch = 3 train_loss = 0.06735992431640625 valid_loss = 0.08390887975692748 valid_acc= 0.9781250357627869: 100%|██████████| 62/62 [00:01<00:00, 53.03it/s]\n",
      "epoch = 4 train_loss = 0.09627187997102737 valid_loss = 0.059655533234278364 valid_acc= 0.9822916984558105: 100%|██████████| 62/62 [00:01<00:00, 52.28it/s]\n",
      "epoch = 5 train_loss = 0.023005859926342964 valid_loss = 0.06748668439686298 valid_acc= 0.9807292222976685: 100%|██████████| 62/62 [00:01<00:00, 52.87it/s]\n",
      "epoch = 6 train_loss = 0.027156878262758255 valid_loss = 0.07560709752142429 valid_acc= 0.9765625596046448: 100%|██████████| 62/62 [00:01<00:00, 50.91it/s]\n",
      "epoch = 7 train_loss = 0.01921946369111538 valid_loss = 0.04364939831818144 valid_acc= 0.9895833730697632: 100%|██████████| 62/62 [00:01<00:00, 51.70it/s]\n",
      "epoch = 8 train_loss = 0.009629804641008377 valid_loss = 0.04474590575943391 valid_acc= 0.9880208969116211: 100%|██████████| 62/62 [00:01<00:00, 52.39it/s]\n",
      "epoch = 9 train_loss = 0.012793446891009808 valid_loss = 0.05852786566441258 valid_acc= 0.9822916984558105: 100%|██████████| 62/62 [00:01<00:00, 53.90it/s]\n",
      "epoch = 10 train_loss = 0.10114878416061401 valid_loss = 0.05152351415405671 valid_acc= 0.9843750596046448: 100%|██████████| 62/62 [00:01<00:00, 52.63it/s]\n",
      "epoch = 11 train_loss = 0.0035512279719114304 valid_loss = 0.044426252506673335 valid_acc= 0.9890625476837158: 100%|██████████| 62/62 [00:01<00:00, 53.39it/s]\n",
      "epoch = 12 train_loss = 0.02441186085343361 valid_loss = 0.07028489235866194 valid_acc= 0.9859375357627869: 100%|██████████| 62/62 [00:01<00:00, 52.15it/s]\n",
      "epoch = 13 train_loss = 0.01372340228408575 valid_loss = 0.04483908637193963 valid_acc= 0.9901041984558105: 100%|██████████| 62/62 [00:01<00:00, 51.22it/s]\n",
      "epoch = 14 train_loss = 0.0033292423468083143 valid_loss = 0.06709220080326 valid_acc= 0.9822916984558105: 100%|██████████| 62/62 [00:01<00:00, 52.01it/s]\n",
      "epoch = 15 train_loss = 0.0073738121427595615 valid_loss = 0.025502381885113817 valid_acc= 0.9937500357627869: 100%|██████████| 62/62 [00:01<00:00, 52.10it/s]\n",
      "epoch = 16 train_loss = 0.0029707576613873243 valid_loss = 0.04058696071733721 valid_acc= 0.99114590883255: 100%|██████████| 62/62 [00:01<00:00, 52.34it/s]\n",
      "epoch = 17 train_loss = 0.0072477892972528934 valid_loss = 0.02036401536121654 valid_acc= 0.9937500357627869: 100%|██████████| 62/62 [00:01<00:00, 53.00it/s]\n",
      "epoch = 18 train_loss = 0.007876181975007057 valid_loss = 0.023281986095632114 valid_acc= 0.9932292103767395: 100%|██████████| 62/62 [00:01<00:00, 51.79it/s]\n",
      "epoch = 19 train_loss = 0.001074285595677793 valid_loss = 0.057502164806161694 valid_acc= 0.9895833730697632: 100%|██████████| 62/62 [00:01<00:00, 50.06it/s]\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epoches):\n",
    "    i = 0\n",
    "    loss_sum = 0\n",
    "    bar = tqdm(train_loader)\n",
    "    for ii, (data , label) in enumerate(bar):\n",
    "        rnn.train()\n",
    "        data = data.to(device)\n",
    "        label = label.to(device)\n",
    "        out = rnn(data)\n",
    "        loss = criterion(out, label)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        i+=1\n",
    "        loss_sum += loss.item()\n",
    "\n",
    "        if ii == 0:\n",
    "            valid_loss,valid_acc = GETACC(valid_loader)\n",
    "            \n",
    "            bar.set_description(f\"epoch = {epoch} train_loss = {loss_sum/i} valid_loss = {valid_loss} valid_acc= {valid_acc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "test_loss = 1.9298308406557356, test_acc = 0.731026828289032\n"
     ]
    }
   ],
   "source": [
    "test_loss,test_acc = GETACC(test_loader)\n",
    "print(f\"test_loss = {test_loss}, test_acc = {test_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}