{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 原始模型\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import datasets, transforms\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader, Dataset, random_split, Subset\n",
    "from tqdm import tqdm,trange\n",
    "import torch.nn.functional as F\n",
    "from attitudeAngle import Update_IMU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "classes = 5  #分类\n",
    "hidden_dim = 64 # rnn隐藏单元数\n",
    "lr = 0.001 # 学习率\n",
    "epoches = 20 #训练次数\n",
    "batch_size = 128 # 每一个训练批次数量\n",
    "input_dim= 21\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "训练集大小63980， 验证集大小15996\n"
     ]
    }
   ],
   "source": [
    "def create_data_loader():\n",
    "    train_data_path = \"D:\\\\temp\\\\attitudeAngle_windows\"\n",
    "    # train_data_path = \"D:\\\\temp\\\\augment_action_windows\"\n",
    "    datasets = ActionDatasets(train_data_path, transform=torch.tensor, target_transform=torch.tensor, pick_path='train_angle.pkl')\n",
    "    test_datasets = ActionDatasets(\"D:\\\\temp\\\\yan1_action_windows\\\\attitudeAngle\", transform=torch.tensor, target_transform=torch.tensor, pick_path='test_angle.pkl')\n",
    "    split_rate = 0.8  # 训练集占整个数据集的比例\n",
    "    train_len = int(split_rate * len(datasets))\n",
    "    valid_len = len(datasets) - train_len\n",
    "\n",
    "    train_sets, valid_sets = random_split(datasets, [train_len, valid_len], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "    train_loader = DataLoader(train_sets, batch_size=batch_size, shuffle=True,drop_last=True,pin_memory=True)\n",
    "    test_loader = DataLoader(test_datasets, batch_size=batch_size, shuffle=True,drop_last=True,pin_memory=True)\n",
    "    valid_loader = DataLoader(valid_sets, batch_size=batch_size, shuffle=True,drop_last=True,pin_memory=True)\n",
    "\n",
    "    print(f\"训练集大小{len(train_sets)}， 验证集大小{len(valid_sets)}\")\n",
    "    return train_loader, test_loader, valid_loader\n",
    "train_loader, test_loader, valid_loader = create_data_loader()"
   ]
  },
  {
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, out_dim):\n",
    "        super(RNN, self).__init__()\n",
    "        self.rnn = nn.LSTM(hidden_dim, hidden_dim, 3, batch_first=True)\n",
    "        self.dropout1 = nn.Dropout(p=0.3)\n",
    "        self.dropout2 = nn.Dropout(p=0.3)\n",
    "\n",
    "        self.linear1 = nn.Linear(hidden_dim, 64)\n",
    "        self.linear2 = nn.Linear(64, out_dim)\n",
    "        self.linear3 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.linear4 = nn.Linear(hidden_dim, hidden_dim)\n",
    "    def forward(self, X):\n",
    "        out = self.linear3(X)\n",
    "        out = self.dropout2(self.linear4(out))\n",
    "\n",
    "        out, status = self.rnn(out)\n",
    "        out = F.relu(out[:,-1,:])\n",
    "        out = F.relu(self.linear1(out))\n",
    "        out = self.dropout1(out)\n",
    "        out = self.linear2(out)\n",
    "\n",
    "        return out"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = RNN(input_dim, hidden_dim, classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(rnn.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GETACC(loader=valid_loader):\n",
    "    rnn.eval()\n",
    "    cnt = 0\n",
    "    sum_valid_acc = 0\n",
    "    sum_valid_loss = 0\n",
    "    for data, label in loader:\n",
    "        data = data.to(device)\n",
    "        label = label.to(device)\n",
    "        out = rnn(data)\n",
    "        \n",
    "        _, predict = torch.max(out, 1)\n",
    "        \n",
    "        loss = criterion(out, label)\n",
    "        sum_valid_loss += loss.item()\n",
    "        acc = torch.sum((predict == label).int()) / batch_size\n",
    "        sum_valid_acc += acc\n",
    "        cnt+=1\n",
    "    \n",
    "    return sum_valid_loss/cnt, sum_valid_acc/cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "epoch = 0 train_loss = 1.6101932525634766 valid_loss = 1.6096146770061985 valid_acc= 0.19657257199287415: 100%|██████████| 499/499 [00:12<00:00, 41.30it/s]\n",
      "epoch = 1 train_loss = 0.8470474481582642 valid_loss = 0.7795671460128599 valid_acc= 0.6719380021095276: 100%|██████████| 499/499 [00:10<00:00, 49.62it/s]\n",
      "epoch = 2 train_loss = 0.7073093056678772 valid_loss = 0.5563744417121333 valid_acc= 0.7854082584381104: 100%|██████████| 499/499 [00:10<00:00, 48.02it/s]\n",
      "epoch = 3 train_loss = 0.4376150965690613 valid_loss = 0.4670838504548996 valid_acc= 0.8266758918762207: 100%|██████████| 499/499 [00:10<00:00, 48.89it/s]\n",
      "epoch = 4 train_loss = 0.3327849507331848 valid_loss = 0.4152368447713314 valid_acc= 0.8422378897666931: 100%|██████████| 499/499 [00:10<00:00, 48.70it/s]\n",
      "epoch = 5 train_loss = 0.4734743535518646 valid_loss = 0.39862948932474657 valid_acc= 0.8448840379714966: 100%|██████████| 499/499 [00:09<00:00, 50.38it/s]\n",
      "epoch = 6 train_loss = 0.4232863783836365 valid_loss = 0.3454663337719056 valid_acc= 0.8649193048477173: 100%|██████████| 499/499 [00:10<00:00, 48.73it/s]\n",
      "epoch = 7 train_loss = 0.447965145111084 valid_loss = 0.353840374898526 valid_acc= 0.8625881671905518: 100%|██████████| 499/499 [00:10<00:00, 49.26it/s]\n",
      "epoch = 8 train_loss = 0.27402234077453613 valid_loss = 0.3363738194588692 valid_acc= 0.8675655126571655: 100%|██████████| 499/499 [00:10<00:00, 49.53it/s]\n",
      "epoch = 9 train_loss = 0.2807392477989197 valid_loss = 0.32739400406998975 valid_acc= 0.8710307478904724: 100%|██████████| 499/499 [00:10<00:00, 48.79it/s]\n",
      "epoch = 10 train_loss = 0.3121066391468048 valid_loss = 0.33000284301177146 valid_acc= 0.8686365485191345: 100%|██████████| 499/499 [00:10<00:00, 48.64it/s]\n",
      "epoch = 11 train_loss = 0.32876965403556824 valid_loss = 0.3049539648477108 valid_acc= 0.876953125: 100%|██████████| 499/499 [00:09<00:00, 50.17it/s]\n",
      "epoch = 12 train_loss = 0.27732521295547485 valid_loss = 0.297571036243631 valid_acc= 0.8795362710952759: 100%|██████████| 499/499 [00:10<00:00, 49.56it/s]\n",
      "epoch = 13 train_loss = 0.18937353789806366 valid_loss = 0.3106514718984404 valid_acc= 0.8746219277381897: 100%|██████████| 499/499 [00:10<00:00, 48.78it/s]\n",
      "epoch = 14 train_loss = 0.32357895374298096 valid_loss = 0.2973462567935067 valid_acc= 0.8804813027381897: 100%|██████████| 499/499 [00:10<00:00, 49.48it/s]\n",
      "epoch = 15 train_loss = 0.2401709258556366 valid_loss = 0.2901965293192094 valid_acc= 0.8801662921905518: 100%|██████████| 499/499 [00:10<00:00, 49.62it/s]\n",
      "epoch = 16 train_loss = 0.29547804594039917 valid_loss = 0.2931367239404109 valid_acc= 0.8792212605476379: 100%|██████████| 499/499 [00:10<00:00, 48.29it/s]\n",
      "epoch = 17 train_loss = 0.3492986261844635 valid_loss = 0.2967083563487376 valid_acc= 0.8804813027381897: 100%|██████████| 499/499 [00:10<00:00, 49.53it/s]\n",
      "epoch = 18 train_loss = 0.25164544582366943 valid_loss = 0.2822230237746431 valid_acc= 0.8835055232048035: 100%|██████████| 499/499 [00:10<00:00, 49.36it/s]\n",
      "epoch = 19 train_loss = 0.33393311500549316 valid_loss = 0.28874234566765444 valid_acc= 0.8807333111763: 100%|██████████| 499/499 [00:10<00:00, 48.40it/s]\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epoches):\n",
    "    i = 0\n",
    "    loss_sum = 0\n",
    "    bar = tqdm(train_loader)\n",
    "    for ii, (data , label) in enumerate(bar):\n",
    "        rnn.train()\n",
    "        print(data.shape, label.shape)\n",
    "        data = data.to(device)\n",
    "        label = label.to(device)\n",
    "        out = rnn(data)\n",
    "        loss = criterion(out, label)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        i+=1\n",
    "        loss_sum += loss.item()\n",
    "\n",
    "        if ii == 0:\n",
    "            valid_loss,valid_acc = GETACC(valid_loader)\n",
    "            \n",
    "            bar.set_description(f\"epoch = {epoch} train_loss = {loss_sum/i} valid_loss = {valid_loss} valid_acc= {valid_acc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "test_loss = 1.9610324927738734, test_acc = 0.3761160969734192\n"
     ]
    }
   ],
   "source": [
    "test_loss,test_acc = GETACC(test_loader)\n",
    "print(f\"test_loss = {test_loss}, test_acc = {test_acc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}